#version 450

layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;

layout(std430, binding = 0) buffer InBufferA { float a[]; };
layout(std430, binding = 1) buffer InBufferB { float b[]; };
layout(std430, binding = 2) buffer OutBuffer { float c[]; };

// Use push constants for metadata to avoid UBO allocations. We pack dimensions
// as u16 pairs into u32 words so we can support up to 6 dims while staying
// within push-constant size limits.
layout(push_constant) uniform PushConstants {
    uint dim_count_a;
    uint dim_count_b;
    uint dim_count_c;

    // Key dimensions for matrix multiplication
    uint m;              // Output rows (from A)
    uint k;              // Contraction dimension
    uint n;              // Output columns (from B)
    
    uint batch_dims;

    // Contraction axis information
    uint a_k_axis;       // Which axis in A is the k dimension
    uint b_k_axis;       // Which axis in B is the k dimension
    uint a_m_axis;       // Which axis in A is the m dimension
    uint b_n_axis;       // Which axis in B is the n dimension

    // Tensor shapes and strides packed as u16 pairs (support up to 6 dims -> 3 u32s)
    uint shape_a_packed[3];
    uint shape_b_packed[3];
    uint shape_c_packed[3];
    uint strides_a_packed[3];
    uint strides_b_packed[3];
    uint strides_c_packed[3];
} dims;

// Shared-memory tiles
shared float tileA[gl_WorkGroupSize.y][gl_WorkGroupSize.x];
shared float tileB[gl_WorkGroupSize.y][gl_WorkGroupSize.x];

// Broadcast-safe indexing: only treat size==1 as broadcast
// Support up to 6 dims packed as two u16 per u32 word
const uint MAX_DIMS = 6u;

// Indexing helper that operates on pre-unpacked shapes/strides. Unpacking
// the packed push-constant words once at the start of the shader reduces
// repeated bit ops and division in the hot loops.
uint idx_broadcast_unpacked(const uint idxs[6], uint dim_count,
                            const uint shape_unpacked[6], const uint strides_unpacked[6]) {
    uint off = 0u;
    uint loop_limit = dim_count;
    if (loop_limit > MAX_DIMS) loop_limit = MAX_DIMS;
    for (uint d = 0u; d < loop_limit; ++d) {
        uint shape_val = shape_unpacked[d];
        uint stride_val = strides_unpacked[d];
        uint dimIdx = (shape_val == 1u) ? 0u : idxs[d];
        off += dimIdx * stride_val;
    }
    return off;
}

void main() {
    uint row = gl_GlobalInvocationID.y;
    uint col = gl_GlobalInvocationID.x;
    uint batchID = gl_GlobalInvocationID.z;


    if (row >= dims.m || col >= dims.n) return;

    // Pre-unpack all packed shape/stride words into small arrays for faster
    // repeated indexing inside the tiled matmul loops. Do this before any use
    // (e.g., computing batch size) so we don't have to unpack repeatedly.
    uint shape_a[6];
    uint shape_b[6];
    uint shape_c[6];
    uint strides_a[6];
    uint strides_b[6];
    uint strides_c[6];
    // Unpack helper: each packed word contains two u16 dims
    for (uint d = 0u; d < MAX_DIMS; ++d) {
        uint wi = d / 2u;
        uint packed;
        // shapes
        packed = dims.shape_a_packed[wi];
        shape_a[d] = ((d & 1u) == 0u) ? (packed & 0xFFFFu) : ((packed >> 16) & 0xFFFFu);
        packed = dims.shape_b_packed[wi];
        shape_b[d] = ((d & 1u) == 0u) ? (packed & 0xFFFFu) : ((packed >> 16) & 0xFFFFu);
        packed = dims.shape_c_packed[wi];
        shape_c[d] = ((d & 1u) == 0u) ? (packed & 0xFFFFu) : ((packed >> 16) & 0xFFFFu);
        // strides
        packed = dims.strides_a_packed[wi];
        strides_a[d] = ((d & 1u) == 0u) ? (packed & 0xFFFFu) : ((packed >> 16) & 0xFFFFu);
        packed = dims.strides_b_packed[wi];
        strides_b[d] = ((d & 1u) == 0u) ? (packed & 0xFFFFu) : ((packed >> 16) & 0xFFFFu);
        packed = dims.strides_c_packed[wi];
        strides_c[d] = ((d & 1u) == 0u) ? (packed & 0xFFFFu) : ((packed >> 16) & 0xFFFFu);
    }

    // Compute total batch size
    uint batch_size = 1u;
    uint loop_limit = dims.batch_dims;
    if (loop_limit > MAX_DIMS) loop_limit = MAX_DIMS;
    for (uint d = 0u; d < loop_limit; ++d) {
        uint sh = shape_c[d];
        batch_size *= sh;
    }
    if (batchID >= batch_size) return;

    // Decode batch indices
    uint rem = batchID;
    uint batch_idxs[6];
    for (uint d = 0u; d < dims.batch_dims && d < MAX_DIMS; ++d) {
        uint wi = d / 2u;
        // unpack on demand for batch decoding
        uint sh = ((d & 1u) == 0u) ? (dims.shape_c_packed[wi] & 0xFFFFu) : ((dims.shape_c_packed[wi] >> 16) & 0xFFFFu);
        batch_idxs[d] = rem % sh;
        rem /= sh;
    }

    // Build base c_indices
    uint c_idxs[6];
    for (uint d = 0u; d < dims.batch_dims && d < MAX_DIMS; ++d)
        c_idxs[d] = batch_idxs[d];
    c_idxs[dims.dim_count_c-2u] = row;
    c_idxs[dims.dim_count_c-1u] = col;
    uint c_index = idx_broadcast_unpacked(c_idxs, dims.dim_count_c, shape_c, strides_c);

    float sum = 0.0;
    uint numTiles = (dims.k + gl_WorkGroupSize.x - 1) / gl_WorkGroupSize.x;

    // Temporary index arrays
    uint a_idxs[6];
    uint b_idxs[6];

    // Loop over tiles in k dimension
    for (uint t = 0; t < numTiles; ++t) {
        // Each thread loads one element of A and B into shared memory
    uint kGlobal = t * gl_WorkGroupSize.x + gl_LocalInvocationID.x;
        // Prepare A indices for broadcast
        for (uint d = 0u; d < dims.batch_dims && d < MAX_DIMS; ++d) a_idxs[d] = batch_idxs[d];
        // Set matrix dims in A
        for (uint d = dims.batch_dims; d < dims.dim_count_a; ++d) {
            if (d == dims.a_m_axis) a_idxs[d] = row;
            else if (d == dims.a_k_axis) a_idxs[d] = (kGlobal < dims.k ? kGlobal : 0u);
            else a_idxs[d] = 0u;
        }
    uint aIndex = idx_broadcast_unpacked(a_idxs, dims.dim_count_a, shape_a, strides_a);
        tileA[gl_LocalInvocationID.y][gl_LocalInvocationID.x] =
            (kGlobal < dims.k ? a[aIndex] : 0.0);

        // Prepare B indices
        for (uint d = 0u; d < dims.batch_dims && d < MAX_DIMS; ++d) b_idxs[d] = batch_idxs[d];
        for (uint d = dims.batch_dims; d < dims.dim_count_b; ++d) {
            if (d == dims.b_k_axis) b_idxs[d] = (kGlobal < dims.k ? kGlobal : 0u);
            else if (d == dims.b_n_axis) b_idxs[d] = col;
            else b_idxs[d] = 0u;
        }
    uint bIndex = idx_broadcast_unpacked(b_idxs, dims.dim_count_b, shape_b, strides_b);
        tileB[gl_LocalInvocationID.y][gl_LocalInvocationID.x] =
            (kGlobal < dims.k ? b[bIndex] : 0.0);

        // Wait for all loads
        barrier();

        // Compute partial sums
        for (uint kk = 0; kk < gl_WorkGroupSize.x; ++kk) {
            sum += tileA[gl_LocalInvocationID.y][kk] *
                   tileB[kk][gl_LocalInvocationID.x];
        }
        barrier();
    }

    // Write out
    c[c_index] = sum;
}
